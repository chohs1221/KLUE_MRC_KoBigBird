{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! apt-get install -y openjdk-8-jdk python3-dev\n",
    "# ! pip install konlpy \"tweepy<4.0.0\"\n",
    "# ! /bin/bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.2 64-bit' requires ipykernel package.\n",
      "Run the following command to install 'ipykernel' into the Python environment. \n",
      "Command: '/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "from typing import List, Tuple, Dict, Any\n",
    "import json\n",
    "import random\n",
    "\n",
    "class KoMRC:\n",
    "    def __init__(self, data, indices: List[Tuple[int, int, int]]):\n",
    "        self._data = data\n",
    "        self._indices = indices\n",
    "\n",
    "    # Json을 불러오는 메소드\n",
    "    @classmethod\n",
    "    def load(cls, file_path: str):\n",
    "        with open(file_path, 'r', encoding='utf-8') as fd:\n",
    "            data = json.load(fd)\n",
    "\n",
    "        indices = []\n",
    "        for d_id, document in enumerate(data['data']):\n",
    "            for p_id, paragraph in enumerate(document['paragraphs']):\n",
    "                for q_id, _ in enumerate(paragraph['qas']):\n",
    "                    indices.append((d_id, p_id, q_id))\n",
    "        \n",
    "        return cls(data, indices)\n",
    "\n",
    "    # 데이터 셋을 잘라내는 메소드\n",
    "    @classmethod\n",
    "    def split(cls, dataset, eval_ratio: float=.1, seed=42):\n",
    "        indices = list(dataset._indices)\n",
    "        random.seed(seed)\n",
    "        random.shuffle(indices)\n",
    "        train_indices = indices[int(len(indices) * eval_ratio):]\n",
    "        eval_indices = indices[:int(len(indices) * eval_ratio)]\n",
    "\n",
    "        return cls(dataset._data, train_indices), cls(dataset._data, eval_indices)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        d_id, p_id, q_id = self._indices[index]\n",
    "        paragraph = self._data['data'][d_id]['paragraphs'][p_id]\n",
    "\n",
    "        context = paragraph['context']\n",
    "        qa = paragraph['qas'][q_id]\n",
    "\n",
    "        guid = qa['guid']\n",
    "        question = qa['question']\n",
    "        answers = qa['answers']\n",
    "\n",
    "        return {\n",
    "            'guid': guid,\n",
    "            'context': context,\n",
    "            'question': question,\n",
    "            'answers': answers\n",
    "        }\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = KoMRC.load('/datasets/train.json')\n",
    "print(\"Number of Samples:\", len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset, dev_dataset = KoMRC.split(dataset)\n",
    "print(\"Number of Train Samples:\", len(train_dataset))\n",
    "print(\"Number of Dev Samples:\", len(dev_dataset))\n",
    "print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "\n",
    "import konlpy\n",
    "\n",
    "class TokenizedKoMRC(KoMRC):\n",
    "    def __init__(self, data, indices: List[Tuple[int, int, int]]) -> None:\n",
    "        super().__init__(data, indices)\n",
    "        self._tagger = konlpy.tag.Mecab()\n",
    "\n",
    "    def _tokenize_with_position(self, sentence: str) -> List[Tuple[str, Tuple[int, int]]]:\n",
    "        position = 0\n",
    "        tokens = []\n",
    "        for morph in self._tagger.morphs(sentence):\n",
    "            position = sentence.find(morph, position)\n",
    "            tokens.append((morph, (position, position + len(morph))))\n",
    "            position += len(morph)\n",
    "        return tokens\n",
    "            \n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        sample = super().__getitem__(index)\n",
    "\n",
    "        context, position = zip(*self._tokenize_with_position(sample['context']))\n",
    "        context, position = list(context), list(position)\n",
    "        question = self._tagger.morphs(sample['question'])\n",
    "\n",
    "        if sample['answers'] is not None:\n",
    "            answers = []\n",
    "            for answer in sample['answers']:\n",
    "                for start, (position_start, position_end) in enumerate(position):\n",
    "                    if position_start <= answer['answer_start'] < position_end:\n",
    "                        break\n",
    "                else:\n",
    "                    print(context, answer)\n",
    "                    raise ValueError(\"No mathced start position\")\n",
    "\n",
    "                target = ''.join(answer['text'].split(' '))\n",
    "                source = ''\n",
    "                for end, morph in enumerate(context[start:], start):\n",
    "                    source += morph\n",
    "                    if target in source:\n",
    "                        break\n",
    "                else:\n",
    "                    print(context, answer)\n",
    "                    raise ValueError(\"No Matched end position\")\n",
    "\n",
    "                answers.append({\n",
    "                    'start': start,\n",
    "                    'end': end\n",
    "                })\n",
    "        else:\n",
    "            answers = None\n",
    "        \n",
    "        return {\n",
    "            'guid': sample['guid'],\n",
    "            'context_original': sample['context'],\n",
    "            'context_position': position,\n",
    "            'question_original': sample['question'],\n",
    "            'context': context,\n",
    "            'question': question,\n",
    "            'answers': answers\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-3-korean-mrc/train.json')\n",
    "\n",
    "train_dataset, dev_dataset = TokenizedKoMRC.split(dataset)\n",
    "print(\"Number of Train Samples:\", len(train_dataset))\n",
    "print(\"Number of Dev Samples:\", len(dev_dataset))\n",
    "print(dev_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = dev_dataset[0]\n",
    "print(sample['context'][sample['answers'][0]['start']:sample['answers'][0]['end']+1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vocab 생성 및 Indexing\n",
    "토큰화된 데이터 셋을 기준으로 Vocab을 만들고 인덱싱을 하는 Indexer를 만들자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Sequence\n",
    "from collections import Counter\n",
    "from itertools import chain\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "class Indexer:\n",
    "    def __init__(self,\n",
    "        id2token: List[str], \n",
    "        max_length: int=1024,\n",
    "        pad: str='<pad>', unk: str='<unk>', cls: str='<cls>', sep: str='<sep>'\n",
    "    ):\n",
    "        self.pad = pad\n",
    "        self.unk = unk\n",
    "        self.cls = cls\n",
    "        self.sep = sep\n",
    "        self.special_tokens = [pad, unk, cls, sep]\n",
    "\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.id2token = self.special_tokens + id2token\n",
    "        self.token2id = {token: token_id for token_id, token in enumerate(self.id2token)}\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.id2token)\n",
    "    \n",
    "    @property\n",
    "    def pad_id(self):\n",
    "        return self.token2id[self.pad]\n",
    "    @property\n",
    "    def unk_id(self):\n",
    "        return self.token2id[self.unk]\n",
    "    @property\n",
    "    def cls_id(self):\n",
    "        return self.token2id[self.cls]\n",
    "    @property\n",
    "    def sep_id(self):\n",
    "        return self.token2id[self.sep]\n",
    "\n",
    "    @classmethod\n",
    "    def build_vocab(cls,\n",
    "        dataset: TokenizedKoMRC, \n",
    "        min_freq: int=5\n",
    "    ):\n",
    "        counter = Counter(chain.from_iterable(\n",
    "            sample['context'] + sample['question']\n",
    "            for sample in tqdm(dataset, desc=\"Counting Vocab\")\n",
    "        ))\n",
    "\n",
    "        return cls([word for word, count in counter.items() if count >= min_freq])\n",
    "    \n",
    "    def decode(self,\n",
    "        token_ids: Sequence[int]\n",
    "    ):\n",
    "        return [self.id2token[token_id] for token_id in token_ids]\n",
    "\n",
    "    def sample2ids(self,\n",
    "        sample: Dict[str, Any],\n",
    "    ) -> Dict[str, Any]:\n",
    "        context = [self.token2id.get(token, self.unk_id) for token in sample['context']]\n",
    "        question = [self.token2id.get(token, self.unk_id) for token in sample['question']]\n",
    "\n",
    "        context = context[:self.max_length-len(question)-3]             # Truncate context\n",
    "        \n",
    "        input_ids = [self.cls_id] + question + [self.sep_id] + context + [self.sep_id]\n",
    "        token_type_ids = [0] * (len(question) + 1) + [1] * (len(context) + 2)\n",
    "\n",
    "        if sample['answers'] is not None:\n",
    "            answer = sample['answers'][0]\n",
    "            start = min(answer['start'] + len(question) + 2, self.max_length - 1)\n",
    "            end = min(answer['end'] + len(question) + 2, self.max_length - 1)\n",
    "        else:\n",
    "            start = None\n",
    "            end = None\n",
    "\n",
    "        return {\n",
    "            'guid': sample['guid'],\n",
    "            'context': sample['context_original'],\n",
    "            'question': sample['question_original'],\n",
    "            'position': sample['context_position'],\n",
    "            'input_ids': input_ids,\n",
    "            'token_type_ids': token_type_ids,\n",
    "            'start': start,\n",
    "            'end': end\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer = Indexer.build_vocab(dataset)\n",
    "print(indexer.sample2ids(dev_dataset[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IndexerWrappedDataset:\n",
    "    def __init__(self, dataset: TokenizedKoMRC, indexer: Indexer) -> None:\n",
    "        self._dataset = dataset\n",
    "        self._indexer = indexer\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._dataset)\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        sample = self._indexer.sample2ids(self._dataset[index])\n",
    "        sample['attention_mask'] = [1] * len(sample['input_ids'])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indexed_train_dataset = IndexerWrappedDataset(train_dataset, indexer)\n",
    "indexed_dev_dataset = IndexerWrappedDataset(dev_dataset, indexer)\n",
    "\n",
    "sample = indexed_dev_dataset[0]\n",
    "print(sample['input_ids'], sample['attention_mask'], sample['token_type_ids'], sample['start'], sample['end'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer Encoder를 활용한 MRC 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from transformers.models.bert.modeling_bert import (\n",
    "    BertModel,\n",
    "    BertPreTrainedModel\n",
    ")\n",
    "\n",
    "## Simple Version for Bert QA: https://huggingface.co/transformers/_modules/transformers/models/bert/modeling_bert.html#BertForQuestionAnswering.forward\n",
    "class BertForQuestionAnswering(BertPreTrainedModel):\n",
    "    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.bert = BertModel(config, add_pooling_layer=False)\n",
    "        self.start_linear = nn.Linear(config.hidden_size, 1)\n",
    "        self.end_linear = nn.Linear(config.hidden_size, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None\n",
    "    ):\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "        )\n",
    "\n",
    "        start_logits = self.start_linear(outputs.last_hidden_state).squeeze(-1)\n",
    "        end_logits = self.end_linear(outputs.last_hidden_state).squeeze(-1)\n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class Collator:\n",
    "    def __init__(self, indexer: Indexer) -> None:\n",
    "        self._indexer = indexer\n",
    "\n",
    "    def __call__(self, samples: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
    "        samples = {\n",
    "            key: [sample[key] for sample in samples]\n",
    "            for key in samples[0]\n",
    "        }\n",
    "\n",
    "        for key in 'start', 'end':\n",
    "            if samples[key][0] is None:\n",
    "                samples[key] = None\n",
    "            else:\n",
    "                samples[key] = torch.tensor(samples[key], dtype=torch.long)\n",
    "        for key in 'input_ids', 'attention_mask', 'token_type_ids':\n",
    "            samples[key] = pad_sequence(\n",
    "                [torch.tensor(sample, dtype=torch.long) for sample in samples[key]],\n",
    "                batch_first=True, padding_value=self._indexer.pad_id\n",
    "            )\n",
    "\n",
    "        return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "batch_size = 64\n",
    "accumulation = 4 # 메모리를 아끼기 위하여 Gradient accumulation을 해보자\n",
    "\n",
    "collator = Collator(indexer)\n",
    "train_loader = DataLoader(indexed_train_dataset, batch_size=batch_size//accumulation, shuffle=True, collate_fn=collator, num_workers=2)\n",
    "dev_loader = DataLoader(indexed_dev_dataset, batch_size=batch_size//accumulation, shuffle=False, collate_fn=collator, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dev_loader))\n",
    "print(batch['input_ids'].shape)\n",
    "print(batch['input_ids'])\n",
    "print(list(batch.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertConfig\n",
    "\n",
    "torch.manual_seed(42)\n",
    "config = BertConfig(\n",
    "     vocab_size=indexer.vocab_size,\n",
    "     max_position_embeddings=1024,\n",
    "     hidden_size=256,\n",
    "     num_hidden_layers=4,\n",
    "     num_attention_heads=4,\n",
    "     intermediate_size=1024\n",
    ")\n",
    "model = BertForQuestionAnswering(config)\n",
    "# model.cuda()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from statistics import mean\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "os.makedirs('dump', exist_ok=True)\n",
    "train_losses = []\n",
    "dev_losses = []\n",
    "\n",
    "step = 0\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    print(\"Epoch\", epoch)\n",
    "    # Training\n",
    "    running_loss = 0.\n",
    "    losses = []\n",
    "    progress_bar = tqdm(train_loader, desc='Train')\n",
    "    for batch in progress_bar:\n",
    "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "        # batch = {key: value.cuda() for key, value in batch.items()}\n",
    "        start = batch.pop('start')\n",
    "        end = batch.pop('end')\n",
    "        \n",
    "        start_logits, end_logits = model(**batch)\n",
    "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "        (loss / accumulation).backward()\n",
    "        running_loss += loss.item()\n",
    "        del batch, start, end, start_logits, end_logits, loss\n",
    "        \n",
    "        step += 1\n",
    "        if step % accumulation:\n",
    "            continue\n",
    "\n",
    "        clip_grad_norm_(model.parameters(), max_norm=1.)\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        losses.append(running_loss / accumulation)\n",
    "        running_loss = 0.\n",
    "        progress_bar.set_description(f\"Train - Loss: {losses[-1]:.3f}\")\n",
    "    train_losses.append(mean(losses))\n",
    "    print(f\"train score: {train_losses[-1]:.3f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    losses = []\n",
    "    for batch in tqdm(dev_loader, desc=\"Evaluation\"):\n",
    "        del batch['guid'], batch['context'], batch['question'], batch['position']\n",
    "        # batch = {key: value.cuda() for key, value in batch.items()}\n",
    "        start = batch.pop('start')\n",
    "        end = batch.pop('end')\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            start_logits, end_logits = model(**batch)\n",
    "        loss = F.cross_entropy(start_logits, start) + F.cross_entropy(end_logits, end)\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        del batch, start, end, start_logits, end_logits, loss\n",
    "    dev_losses.append(mean(losses))\n",
    "    print(f\"Evaluation score: {dev_losses[-1]:.3f}\")\n",
    "\n",
    "    model.save_pretrained(f'dump/model.{epoch}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t = list(range(1, 31))\n",
    "plt.plot(t, train_losses, label=\"Train Loss\")\n",
    "plt.plot(t, dev_losses, label=\"Dev Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForQuestionAnswering.from_pretrained('dump/model.30')\n",
    "model.cuda()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, sample in zip(range(1, 4), indexed_train_dataset):\n",
    "    print(f'------{idx}------')\n",
    "    print('Context:', sample['context'])\n",
    "    print('Question:', sample['question'])\n",
    "    \n",
    "    input_ids, token_type_ids = [\n",
    "        torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n",
    "        for key in (\"input_ids\", \"token_type_ids\")\n",
    "    ]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n",
    "    start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "    \n",
    "    start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
    "    end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
    "    probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n",
    "    index = torch.argmax(probability).item()\n",
    "    \n",
    "    start = index // len(end_prob)\n",
    "    end = index % len(end_prob)\n",
    "    \n",
    "    start = sample['position'][start][0]\n",
    "    end = sample['position'][end][1]\n",
    "\n",
    "    print('Answer:', sample['context'][start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test 출력 파일 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = TokenizedKoMRC.load('/kaggle/input/k-digital-goorm-3-korean-mrc/test.json')\n",
    "test_dataset = IndexerWrappedDataset(test_dataset, indexer)\n",
    "print(\"Number of Test Samples\", len(test_dataset))\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "os.makedirs('out', exist_ok=True)\n",
    "with torch.no_grad(), open('out/baseline.csv', 'w') as fd:\n",
    "    writer = csv.writer(fd)\n",
    "    writer.writerow(['Id', 'Predicted'])\n",
    "\n",
    "    rows = []\n",
    "    for sample in tqdm(test_dataset, \"Testing\"):\n",
    "        input_ids, token_type_ids = [\n",
    "            torch.tensor(sample[key], dtype=torch.long, device=\"cuda\")\n",
    "            for key in (\"input_ids\", \"token_type_ids\")\n",
    "        ]\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            start_logits, end_logits = model(input_ids=input_ids[None, :], token_type_ids=token_type_ids[None, :])\n",
    "        start_logits.squeeze_(0), end_logits.squeeze_(0)\n",
    "    \n",
    "        start_prob = start_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
    "        end_prob = end_logits[token_type_ids.bool()][1:-1].softmax(-1)\n",
    "        probability = torch.triu(start_prob[:, None] @ end_prob[None, :])\n",
    "        index = torch.argmax(probability).item()\n",
    "    \n",
    "        start = index // len(end_prob)\n",
    "        end = index % len(end_prob)\n",
    "    \n",
    "        start = sample['position'][start][0]\n",
    "        end = sample['position'][end][1]\n",
    "\n",
    "        rows.append([sample[\"guid\"], sample['context'][start:end]])\n",
    "    \n",
    "    writer.writerows(rows)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
